(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[1046],{3905:function(e,t,a){"use strict";a.d(t,{Zo:function(){return p},kt:function(){return c}});var n=a(7294);function i(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){i(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function u(e,t){if(null==e)return{};var a,n,i=function(e,t){if(null==e)return{};var a,n,i={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||(i[a]=e[a]);return i}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(i[a]=e[a])}return i}var l=n.createContext({}),s=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},p=function(e){var t=s(e.components);return n.createElement(l.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var a=e.components,i=e.mdxType,r=e.originalType,l=e.parentName,p=u(e,["components","mdxType","originalType","parentName"]),m=s(a),c=i,f=m["".concat(l,".").concat(c)]||m[c]||d[c]||r;return a?n.createElement(f,o(o({ref:t},p),{},{components:a})):n.createElement(f,o({ref:t},p))}));function c(e,t){var a=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var r=a.length,o=new Array(r);o[0]=m;var u={};for(var l in t)hasOwnProperty.call(t,l)&&(u[l]=t[l]);u.originalType=e,u.mdxType="string"==typeof e?e:i,o[1]=u;for(var s=2;s<r;s++)o[s]=a[s];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}m.displayName="MDXCreateElement"},4513:function(e,t,a){"use strict";a.r(t),a.d(t,{frontMatter:function(){return o},metadata:function(){return u},toc:function(){return l},default:function(){return p}});var n=a(2122),i=a(9756),r=(a(7294),a(3905)),o={title:"visualizeAudio()",id:"visualize-audio"},u={unversionedId:"visualize-audio",id:"visualize-audio",isDocsHomePage:!1,title:"visualizeAudio()",description:"Part of the @remotion/media-utils package of helper functions.",source:"@site/docs/visualize-audio.md",sourceDirName:".",slug:"/visualize-audio",permalink:"/docs/visualize-audio",editUrl:"https://github.com/JonnyBurger/remotion/edit/main/packages/docs/docs/visualize-audio.md",version:"current",frontMatter:{title:"visualizeAudio()",id:"visualize-audio"},sidebar:"someSidebar",previous:{title:"useAudioData()",permalink:"/docs/use-audio-data"},next:{title:"getCompositions()",permalink:"/docs/get-compositions"}},l=[{value:"Arguments",id:"arguments",children:[{value:"<code>options</code>",id:"options",children:[]}]},{value:"Return value",id:"return-value",children:[]},{value:"Example",id:"example",children:[]},{value:"See also",id:"see-also",children:[]}],s={toc:l};function p(e){var t=e.components,a=(0,i.Z)(e,["components"]);return(0,r.kt)("wrapper",(0,n.Z)({},s,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"Part of the ",(0,r.kt)("inlineCode",{parentName:"em"},"@remotion/media-utils"))," package of helper functions."),(0,r.kt)("p",null,"This function takes in ",(0,r.kt)("inlineCode",{parentName:"p"},"AudioData")," (preferrably fetched by the ",(0,r.kt)("a",{parentName:"p",href:"use-audio-data"},(0,r.kt)("inlineCode",{parentName:"a"},"useAudioData()"))," hook) and processes it in a way that makes visualizing the audio that is playing at the current frame easy."),(0,r.kt)("h2",{id:"arguments"},"Arguments"),(0,r.kt)("h3",{id:"options"},(0,r.kt)("inlineCode",{parentName:"h3"},"options")),(0,r.kt)("p",null,"The only argument for this function is an object containing the following values:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"audioData"),": ",(0,r.kt)("inlineCode",{parentName:"p"},"AudioData")," - an object containing audio data. You can fetch this object using ",(0,r.kt)("a",{parentName:"p",href:"use-audio-data"},(0,r.kt)("inlineCode",{parentName:"a"},"useAudioData()"))," or ",(0,r.kt)("a",{parentName:"p",href:"get-audio-data"},(0,r.kt)("inlineCode",{parentName:"a"},"getAudioData()")),".")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"frame"),": ",(0,r.kt)("inlineCode",{parentName:"p"},"number")," - the time of the track that you want to get the audio information for. The ",(0,r.kt)("inlineCode",{parentName:"p"},"frame")," always refers to the position in the audio track - if you have shifted or trimmed the audio in your timeline, the frame returned by ",(0,r.kt)("inlineCode",{parentName:"p"},"useCurrentFrame")," must also be tweaked before you pass it into this function.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"fps"),": ",(0,r.kt)("inlineCode",{parentName:"p"},"number")," - the frame rate of the composition. This helps the function understand the meaning of the ",(0,r.kt)("inlineCode",{parentName:"p"},"frame")," input.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"numberOfSamples"),": ",(0,r.kt)("inlineCode",{parentName:"p"},"number")," - must be a power of two, such as ",(0,r.kt)("inlineCode",{parentName:"p"},"32"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"64"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"128"),", etc. This parameter controls the length of the output array. A lower number will simplify the spectrum and is useful if you want to animate elements roughly based on the level of lows, mids and highs. A higher number will give the spectrum in more detail, which is useful for displaying a bar chart or waveform-style visualization of the audio."))),(0,r.kt)("h2",{id:"return-value"},"Return value"),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"number[]")," - An array of values describing the amplitude of each frequency range. Each value is between 0 and 1. The array is of length defined by the ",(0,r.kt)("inlineCode",{parentName:"p"},"numberOfSamples")," parameter."),(0,r.kt)("p",null,"The values on the left of the array are low frequencies (for example bass) and as we move towards the right, we go through the mid and high frequencies like drums and vocals."),(0,r.kt)("p",null,"Usually the values on left side of the array can become much larger than the values on the right. This is not a mistake, but for some visualizations you might have to apply some postprocessing to it, you can flatten the curve by for example mapping each value to a logarithm of the original function."),(0,r.kt)("h2",{id:"example"},"Example"),(0,r.kt)("p",null,"In this example, we render a bar chart visualizing the audio spectrum of an audio file we imported using ",(0,r.kt)("a",{parentName:"p",href:"use-audio-data"},(0,r.kt)("inlineCode",{parentName:"a"},"useAudioData()"))," and ",(0,r.kt)("inlineCode",{parentName:"p"},"visualizeAudio()"),"."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-tsx"},"import {Audio, useCurrentFrame, useVideoConfig} from 'remotion';\nimport {useAudioData, visualizeAudio} from '@remotion/media-utils';\nimport music from './music.mp3';\n\nexport const MyComponent: React.FC = () => {\n  const frame = useCurrentFrame();\n  const {width, height, fps} = useVideoConfig();\n  const audioData = useAudioData(music);\n\n  if (!audioData) {\n    return null;\n  }\n\n  const visualization = visualizeAudio({\n    fps,\n    frame,\n    audioData,\n    numberOfSamples: 16,\n  }); // [0.22, 0.1, 0.01, 0.01, 0.01, 0.02, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n  // Render a bar chart for each frequency, the higher the amplitude,\n  // the longer the bar\n  return (\n    <div>\n      <Audio src={music}/>\n      {visualization.map(v => {\n        return (\n          <div style={{width: 1000 * v, height: 15, backgroundColor: 'blue'}} />\n        );\n      })}\n    </div>\n  )\n}\n\n")),(0,r.kt)("h2",{id:"see-also"},"See also"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"/docs/audio-visualization"},"Audio visualization")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"/docs/use-audio-data"},(0,r.kt)("inlineCode",{parentName:"a"},"useAudioData()"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"/docs/get-audio-data"},(0,r.kt)("inlineCode",{parentName:"a"},"getAudioData()"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"/docs/audio"},(0,r.kt)("inlineCode",{parentName:"a"},"<Audio/>"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"/docs/using-audio"},"Using audio"))))}p.isMDXComponent=!0}}]);